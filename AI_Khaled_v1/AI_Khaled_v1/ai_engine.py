# -*- coding: utf-8 -*-
"""
ai_engine.py â€” Ù…Ø­Ø±Ùƒ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø°ÙƒÙŠ (Ù…Ø¹Ø¯Ù‘Ù„)
ØªØºÙŠÙŠØ±Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:
- Ø£Ø²Ù„Øª Ø¸Ù‡ÙˆØ± ÙƒÙ„Ù…Ø© "[RETRIEVE]" ÙÙŠ Ø§Ù„Ù„ÙˆØ¬ ÙˆØ§Ø³ØªØ¨Ø¯Ù„ØªÙ‡Ø§ Ø¨Ø¹Ù„Ø§Ù…Ø© Ø£Ø¨Ø³Ø· "[MEM]".
- Ø¹Ø·Ù‘Ù„Øª Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Markov ÙƒÙ€ fallback Ù†Ù‡Ø§Ø¦ÙŠ Ø­ØªÙ‰ Ù…Ø§ ÙŠØ­ØµÙ„Ø´ Ø±Ø¯ÙˆØ¯ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ø«Ù„ "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡".
- Ø­Ø§ÙØ¸Øª Ø¹Ù„Ù‰ Ø¨Ù‚ÙŠØ© Ø§Ù„Ø¨Ù†ÙŠØ© ÙˆØ§Ù„Ù…Ø²Ø§ÙŠØ§ ÙƒÙ…Ø§ Ù‡ÙŠ (KB, dataset, memory, ML, caching, pending).
- Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ØŒ ÙŠÙØ³Ø¬Ù‘ÙÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙƒÙ€ pending ÙˆØªÙØ¹Ø§Ø¯ Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„Ù… ÙÙ‚Ø·.
"""
import os
import json
import csv
import random
import logging
import re
import threading
import pickle
from pathlib import Path
from datetime import datetime

ROOT = Path(__file__).parent
DATA_DIR = ROOT / "data"
MEM_PATH = DATA_DIR / "memory.json"
DS_PATH = DATA_DIR / "dataset.csv"
KB_PATH = DATA_DIR / "kb.json"
MODEL_PATH = ROOT / "model" / "khalid_model.pkl"
CONFIG_PATH = DATA_DIR / "config.json"

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
SIMILARITY_THRESHOLD_RETRIEVE = 0.45
SIMILARITY_THRESHOLD_DATASET = 0.5
MARKOV_N = 2
MARKOV_MAX_LEN = 40
AUTO_RETRAIN_DEFAULT = False

# logging
LOG_PATH = DATA_DIR / "ai_engine.log"
logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] [ai_engine] [%(levelname)s] %(message)s",
    handlers=[logging.FileHandler(LOG_PATH, encoding="utf-8"), logging.StreamHandler()]
)

# ØªØ£ÙƒØ¯ ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
DATA_DIR.mkdir(parents=True, exist_ok=True)
if not MEM_PATH.exists():
    MEM_PATH.write_text(json.dumps({"sessions": []}, ensure_ascii=False, indent=2), encoding="utf-8")
if not DS_PATH.exists():
    with open(DS_PATH, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["question", "answer"])
if not KB_PATH.exists():
    KB_PATH.write_text(json.dumps({"hello":"Ø£Ù‡Ù„Ø§Ù‹! ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ"}, ensure_ascii=False, indent=2), encoding="utf-8")
if not CONFIG_PATH.exists():
    CONFIG_PATH.write_text(json.dumps({"auto_retrain": AUTO_RETRAIN_DEFAULT, "auto_train": True}, ensure_ascii=False, indent=2), encoding="utf-8")

# locks Ù„Ù„Ø­Ù…Ø§ÙŠØ©
_ds_lock = threading.Lock()
_mem_lock = threading.Lock()
_kb_lock = threading.Lock()

# ÙƒØ§Ø´ Ø¨Ø³ÙŠØ·
_reply_cache = {}  # normalized_text -> reply

# pending map Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø°Ø§ØªÙŠ
_pending = {}  # session_id -> question

# ------------------------
# Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø©
# ------------------------
def _now_ts():
    return int(datetime.now().timestamp())

def _clean_text(t: str) -> str:
    if not isinstance(t, str):
        return ""
    t = t.lower().strip()
    # Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª
    t = re.sub(r"[^\u0600-\u06FFa-z0-9\s]", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

def _similarity(a: str, b: str) -> float:
    sa = set(_clean_text(a).split())
    sb = set(_clean_text(b).split())
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / max(len(sa), len(sb))

def _read_json(path: Path, default):
    try:
        return json.load(open(path, "r", encoding="utf-8"))
    except Exception as e:
        logging.warning(f"failed reading {path}: {e}")
        return default

def _write_json(path: Path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ------------------------
# ØªØ­Ù…ÙŠÙ„ KB Ùˆ dataset (ÙƒØ§Ø´)
# ------------------------
_kb_cache = None
_dataset_cache = None

def _load_kb():
    global _kb_cache
    if _kb_cache is None:
        _kb_cache = _read_json(KB_PATH, {})
    return _kb_cache

def _load_dataset():
    global _dataset_cache
    if _dataset_cache is not None:
        return _dataset_cache
    pairs = []
    try:
        with open(DS_PATH, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            header = next(reader, None)
            for row in reader:
                if len(row) >= 2:
                    q = row[0].strip()
                    a = row[1].strip()
                    if q and a:
                        pairs.append((q, a))
    except Exception as e:
        logging.warning(f"failed loading dataset.csv: {e}")
    _dataset_cache = pairs
    logging.info(f"dataset loaded: {len(pairs)} pairs")
    return _dataset_cache

def _refresh_dataset_cache():
    global _dataset_cache
    _dataset_cache = None
    return _load_dataset()

# ------------------------
# memory helpers
# ------------------------
def load_memory():
    return _read_json(MEM_PATH, {"sessions": []})

def save_memory(mem):
    with _mem_lock:
        _write_json(MEM_PATH, mem)

# ------------------------
# Ø­ÙØ¸ Ø²ÙˆØ¬ Ø¬Ø¯ÙŠØ¯ (Ø³Ø¤Ø§Ù„ -> Ø¥Ø¬Ø§Ø¨Ø©)
# ------------------------
def save_new_pair(question: str, answer: str, session_id: str = None):
    question = question.strip()
    answer = answer.strip()
    if not question or not answer:
        return False
    # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    dataset = _load_dataset()
    if (question, answer) in dataset:
        logging.info("pair already exists, skipping save")
        return False
    # Ø§ÙƒØªØ¨ ÙÙŠ CSV
    with _ds_lock:
        try:
            file_exists = DS_PATH.exists()
            with open(DS_PATH, "a", encoding="utf-8", newline="") as f:
                writer = csv.writer(f)
                if not file_exists:
                    writer.writerow(["question", "answer"])
                writer.writerow([question, answer])
            logging.info(f"[LEARN] saved pair: {question} -> {answer}")
            _refresh_dataset_cache()
        except Exception as e:
            logging.warning(f"failed to append dataset: {e}")
            return False
    # Ø£Ø¶Ù Ø£ÙŠØ¶Ø§Ù‹ Ù„Ù„Ø°Ø§ÙƒØ±Ø©
    try:
        mem = load_memory()
        if session_id:
            session = next((s for s in mem.get("sessions", []) if s.get("id") == session_id), None)
        else:
            session = mem.get("sessions")[-1] if mem.get("sessions") else None
        if not session:
            session = {"id": session_id or str(_now_ts()), "messages": []}
            mem.setdefault("sessions", []).append(session)
        session["messages"].append({
            "timestamp": _now_ts(),
            "user_text": question,
            "bot_text": answer
        })
        save_memory(mem)
    except Exception as e:
        logging.warning(f"failed to add to memory: {e}")
    # ØªØ­Ø¯ÙŠØ« KB ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ø³ÙŠØ·: Ø¥Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‚ØµÙŠØ±ØŒ Ø¶Ù…Ù‡ ÙƒÙ…ÙØªØ§Ø­
    try:
        if len(question.split()) <= 5:
            with _kb_lock:
                kb = _load_kb()
                if question not in kb:
                    kb[question] = answer
                    _write_json = lambda p,d: open(p,"w",encoding="utf-8").write(json.dumps(d, ensure_ascii=False, indent=2))
                    _write_json(KB_PATH, kb)
                    global _kb_cache
                    _kb_cache = kb
    except Exception as e:
        logging.warning(f"failed to update kb: {e}")

    # Ø®ÙŠØ§Ø± Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¥Ù† ÙƒØ§Ù† Ù…ÙØ¹Ù„Ø§Ù‹
    cfg = _read_json(CONFIG_PATH, {})
    if cfg.get("auto_retrain"):
        try:
            threading.Thread(target=lambda: os.system(f'"{os.sys.executable}" "{ROOT/"train.py"}"'), daemon=True).start()
        except Exception as e:
            logging.warning(f"failed to trigger retrain: {e}")

    return True

# ------------------------
# Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
# ------------------------
def retrieve(user_text: str, session_id: str = None):
    mem = load_memory()
    best = None; best_score = 0.0
    for sess in mem.get("sessions", []):
        if session_id and sess.get("id") != session_id:
            continue
        for conv in sess.get("messages", []):
            s = _similarity(user_text, conv.get("user_text", ""))
            if s > best_score:
                best_score = s
                best = conv
    if best and best_score >= SIMILARITY_THRESHOLD_RETRIEVE:
        # ØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ ÙˆØ³Ù… Ø§Ù„Ù„ÙˆØ¬ Ø¥Ù„Ù‰ ÙˆØ³Ù… Ø£Ø¨Ø³Ø· "[MEM]" Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† "[RETRIEVE]"
        logging.info(f"[MEM] score={best_score:.2f}")
        return best.get("bot_text")
    return None

# ------------------------
# dataset lookup (direct match by similarity)
# ------------------------
def dataset_lookup(user_text: str):
    dataset = _load_dataset()
    best_score = 0.0; best_answer = None
    for q,a in dataset:
        s = _similarity(user_text, q)
        if s > best_score:
            best_score = s; best_answer = a
    if best_answer and best_score >= SIMILARITY_THRESHOLD_DATASET:
        logging.info(f"[DATASET] match score={best_score:.2f}")
        return best_answer
    return None

# ------------------------
# KB lookup
# ------------------------
def kb_lookup(user_text: str):
    kb = _load_kb()
    text = _clean_text(user_text)
    for key, val in kb.items():
        if key and _clean_text(key) in text:
            logging.info(f"[KB] matched key='{key}'")
            return val
    return None

# ------------------------
# Markov fallback (n-grams) -- ØªØ§Ø¨Ø¹ Ù…ÙˆØ¬ÙˆØ¯ Ù„ÙƒÙ† Ù„Ù† ÙŠÙØ³ØªØ¯Ø¹Ù‰ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø§Ù„Ø¢Ù†
# ------------------------
def markov_fallback(seed: str, n: int = MARKOV_N, max_len: int = MARKOV_MAX_LEN):
    corpus = []
    # from dataset
    for q,a in _load_dataset():
        corpus.append(q); corpus.append(a)
    # from memory
    mem = load_memory()
    for sess in mem.get("sessions", []):
        for m in sess.get("messages", []):
            corpus.append(m.get("user_text","")); corpus.append(m.get("bot_text",""))
    text = " ".join([c for c in corpus if c])
    tokens = _clean_text(text).split()
    if len(tokens) < n:
        return ""
    trans = {}
    for i in range(len(tokens)-n):
        key = tuple(tokens[i:i+n])
        trans.setdefault(key, []).append(tokens[i+n])
    seed_toks = _clean_text(seed).split()[:n]
    if len(seed_toks) < n:
        key = random.choice(list(trans.keys()))
    else:
        key = tuple(seed_toks)
        if key not in trans:
            key = random.choice(list(trans.keys()))
    out = list(key)
    for _ in range(max_len):
        nxts = trans.get(tuple(out[-n:]))
        if not nxts: break
        out.append(random.choice(nxts))
    return " ".join(out)

# ------------------------
# ML model attempt (safe)
# ------------------------
def try_ml_model(user_text: str):
    try:
        if MODEL_PATH.exists():
            vec, model = pickle.load(open(MODEL_PATH, "rb"))
            Xv = vec.transform([user_text])
            pred = model.predict(Xv)
            if pred and len(pred) > 0:
                logging.info("[ML] model returned an answer")
                return pred[0]
    except Exception as e:
        logging.warning(f"ML error: {e}")
    return None

# ------------------------
# ÙˆØ§Ø¬Ù‡Ø§Øª pending management Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ app.py
# ------------------------
def is_waiting_for_answer(session_id: str) -> bool:
    return session_id in _pending

def provide_answer_for_pending(session_id: str, answer: str):
    """
    ÙŠØ¹Ø§Ù„Ø¬ Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø³Ø¤Ø§Ù„ ØªÙ… Ø·Ù„Ø¨ ØªØ¹Ù„ÙŠÙ…Ù‡ Ù…Ø³Ø¨Ù‚Ø§Ù‹.
    ÙŠØ¹ÙŠØ¯ (True, message) Ù„Ùˆ Ù†Ø¬Ø­ØŒ ÙˆØ¥Ù„Ø§ (False, error_message)
    """
    if session_id not in _pending:
        return False, "Ù…Ø§ ÙƒØ§Ù†Ø´ ÙÙŠÙ‡ Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ†ÙŠ Ø¥Ø¬Ø§Ø¨Ø©."
    question = _pending.pop(session_id)
    ok = save_new_pair(question, answer, session_id=session_id)
    if ok:
        return True, "ØªÙ…Ø§Ù… âœ… Ø­ÙØ¸Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙˆÙ‡ÙØªÙƒØ±Ù‡Ø§ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø¬Ø§ÙŠØ©."
    else:
        return False, "Ù…Ø­ØµÙ„Ø´ Ø­ÙØ¸ â€” Ù…Ù…ÙƒÙ† ØªØ¬Ø±Ø¨ ØªØ§Ù†ÙŠØŸ"

# ------------------------
# Ø§Ù„ÙƒØ§Ø´: Ù‚Ø±Ø§Ø¡Ø© ÙˆÙƒØªØ§Ø¨Ø©
# ------------------------
def _cache_get(user_text: str):
    return _reply_cache.get(_clean_text(user_text))

def _cache_set(user_text: str, reply: str):
    _reply_cache[_clean_text(user_text)] = reply

# ------------------------
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
# ------------------------
def generate_reply(user_text: str, session_id: str = None) -> str:
    """
    ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª:
    1) cache
    2) KB
    3) memory retrieval
    4) dataset lookup
    5) ML model
    6) (Markov Ù…ÙØ¹Ø·Ù‘Ù„ Ù‡Ù†Ø§)
    7) Ask user to teach (register pending)
    """
    if not user_text or not isinstance(user_text, str):
        return "Ù…Ø¹Ù„Ø´ Ù…Ø´ Ù‚Ø§Ø¯Ø± Ø£Ø¬Ø§ÙˆØ¨ Ø¯Ù„ÙˆÙ‚ØªÙŠ."

    # cache
    c = _cache_get(user_text)
    if c:
        logging.info("[CACHE] hit")
        return c

    # KB
    kb_ans = kb_lookup(user_text)
    if kb_ans:
        _cache_set(user_text, kb_ans)
        return kb_ans

    # memory retrieval
    mem_ans = retrieve(user_text, session_id)
    if mem_ans:
        _cache_set(user_text, mem_ans)
        return mem_ans

    # dataset lookup
    ds_ans = dataset_lookup(user_text)
    if ds_ans:
        _cache_set(user_text, ds_ans)
        return ds_ans

    # ML
    ml_ans = try_ml_model(user_text)
    if ml_ans:
        _cache_set(user_text, ml_ans)
        return ml_ans

    # Markov fallback Ù…ÙØ¹Ø·Ù‘Ù„: Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù…Ù‡ Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ø±Ø¯ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    # Ø¥Ø°Ø§ Ù„Ø§ÙŠÙˆØ¬Ø¯ Ø´ÙŠØ¡ Ù…Ù†Ø§Ø³Ø¨ -> Ù†Ø³Ø¬Ù„ ÙƒÙ€ pending ÙˆÙ†Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø§Ø¹Ø¯Ù†Ø§ Ø¨Ø§Ù„ØªØ¹Ù„ÙŠÙ…
    sid = session_id or str(_now_ts())
    _pending[sid] = user_text
    logging.info(f"[TEACH_REQUEST] session={sid} question='{user_text}'")
    return "ğŸ¤” Ù…Ø´ Ù…ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù…Ù…ÙƒÙ† ØªÙ‚ÙˆÙ„Ù‘ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ Ø¹Ù„Ø´Ø§Ù† Ø£ØªØ¹Ù„Ù…Ù‡Ø§ØŸ"

# ------------------------
# init load
# ------------------------
_load_kb = _load_kb  # alias to load at import time
_load_dataset = _load_dataset
_load_kb()
_load_dataset()
logging.info("ai_engine initialized.")
