# -*- coding: utf-8 -*-
"""
train.py â€” ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ
ÙŠÙ‚Ø±Ø£ Ù…Ù† data/dataset.csv Ùˆ data/memory.json
ÙˆÙŠØ­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ model/khalid_model.pkl
ÙŠØªØ¹Ø±Ù ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© (question/answer Ø£Ùˆ user_text/bot_text)
ÙŠØªØ·Ù„Ø¨ scikit-learn
"""
import os, json, pickle, csv
from pathlib import Path

ROOT = Path(__file__).parent
DATA_DIR = ROOT / "data"
DS_PATH = DATA_DIR / "dataset.csv"
MEM_PATH = DATA_DIR / "memory.json"
MODEL_DIR = ROOT / "model"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

pairs = []

# ------------------------
# Load dataset.csv
# ------------------------
if DS_PATH.exists():
    try:
        with open(DS_PATH, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            count = 0
            for row in reader:
                # ÙŠØ¯Ø¹Ù… ÙƒÙ„Ø§ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ÙŠÙ†
                q = row.get("question") or row.get("user_text") or ""
                a = row.get("answer") or row.get("bot_text") or ""
                q, a = q.strip(), a.strip()
                if q and a:
                    pairs.append((q, a))
                    count += 1
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {count} Ø²ÙˆØ¬ Ù…Ù† dataset.csv")
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© dataset.csv: {e}")
else:
    print("âš ï¸ Ù…Ù„Ù dataset.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø¹Ø¯.")

# ------------------------
# Load memory.json
# ------------------------
if MEM_PATH.exists():
    try:
        mem = json.load(open(MEM_PATH, "r", encoding="utf-8"))
        mem_count = 0
        for sess in mem.get("sessions", []):
            for conv in sess.get("messages", []):
                u = conv.get("user_text", "").strip()
                b = conv.get("bot_text", "").strip()
                if u and b:
                    pairs.append((u, b))
                    mem_count += 1
        print(f"ğŸ§  ØªÙ… Ø¥Ø¶Ø§ÙØ© {mem_count} Ø²ÙˆØ¬ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©")
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: {e}")
else:
    print("âš ï¸ Ù…Ù„Ù memory.json ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø¹Ø¯.")

# ------------------------
# Check data
# ------------------------
if not pairs:
    print("ğŸš« Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨. Ø£Ø¶Ù Ø£Ø³Ø¦Ù„Ø© Ø¥Ù„Ù‰ dataset.csv Ø£Ùˆ ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ø¨ÙˆØª Ø£ÙˆÙ„Ù‹Ø§.")
    exit(0)

X, y = zip(*pairs)
print(f"ğŸ”§ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ {len(pairs)} Ø¬Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©...")

# ------------------------
# Import scikit-learn
# ------------------------
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.neighbors import KNeighborsClassifier
except Exception:
    print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ scikit-learn. Ø«Ø¨Ù‘ØªÙ‡Ø§ Ø¹Ø¨Ø±: pip install scikit-learn")
    exit(1)

# ------------------------
# Train Model
# ------------------------
vec = TfidfVectorizer(analyzer="word", ngram_range=(1, 3))
Xv = vec.fit_transform(X)
model = KNeighborsClassifier(n_neighbors=3)
model.fit(Xv, y)

# ------------------------
# Save model
# ------------------------
model_file = MODEL_DIR / "khalid_model.pkl"
with open(model_file, "wb") as f:
    pickle.dump((vec, model), f)

print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­. ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {model_file}")
